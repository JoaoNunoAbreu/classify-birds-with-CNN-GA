{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense \n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# loading up our datasets\n",
    "train_data_dir = 'dataset/train'\n",
    "validation_data_dir = 'dataset/valid'\n",
    "test_data_dir = 'dataset/test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 7 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Loading vgc16 model\n",
    "vgg16 = applications.VGG16(include_top=False, weights='imagenet')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35215 images belonging to 250 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaonunoabreu/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    " \n",
    "np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    validation_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_valid_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_valid = int(math.ceil(nb_valid_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_valid = vgg16.predict_generator(generator, predict_size_valid) \n",
    " \n",
    "np.save('bottleneck_features_valid.npy', bottleneck_features_valid)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    test_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_test_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_test = vgg16.predict_generator(generator, predict_size_test) \n",
    " \n",
    "np.save('bottleneck_features_test.npy', bottleneck_features_test)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode=’categorical’, \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load(‘bottleneck_features_train.npy’) \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validing data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   valid_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode=’categorical’, \n",
    "   shuffle=False) \n",
    " \n",
    "nb_valid_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "valid_data = np.load(‘bottleneck_features_valid.npy’) \n",
    " \n",
    "# get the class labels for the validing data, in the original order \n",
    "valid_labels = generator_top.classes \n",
    " \n",
    "# convert the validing labels to categorical vectors \n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   test_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode=’categorical’, \n",
    "   shuffle=False) \n",
    " \n",
    "nb_test_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "test_data = np.load(‘bottleneck_features_test.npy’) \n",
    " \n",
    "# get the class labels for the testing data, in the original order \n",
    "test_labels = generator_top.classes \n",
    " \n",
    "# convert the testing labels to categorical vectors \n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "start = datetime.datetime.now()\n",
    "model = Sequential() \n",
    "model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=7,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   print(\"[INFO] loading and preprocessing image…\") \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "def test_single_image(path):\n",
    "    animals = ['butterflies', 'chickens', 'elephants', 'horses', 'spiders', 'squirells']\n",
    "    images = read_image(path)\n",
    "    time.sleep(.5)\n",
    "    bt_prediction = vgg16.predict(images) \n",
    "    preds = model.predict_proba(bt_prediction)\n",
    "    for idx, animal, x in zip(range(0,6), animals , preds[0]):\n",
    "        print(\"ID: {}, Label: {} {}%\".format(idx, animal, round(x*100,2) ))\n",
    "    print('Final Decision:')\n",
    "    time.sleep(.5)\n",
    "    for x in range(3):\n",
    "        print('.'*(x+1))\n",
    "        time.sleep(.2)\n",
    "    class_predicted = model.predict_classes(bt_prediction)\n",
    "    class_dictionary = generator_top.class_indices \n",
    "    inv_map = {v: k for k, v in class_dictionary.items()} \n",
    "    print(\"ID: {}, Label: {}\".format(class_predicted[0],  inv_map[class_predicted[0]])) \n",
    "    return load_img(path)\n",
    "path = 'data/test/yourpicturename'\n",
    "test_single_image(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
